<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial · LsqFit.jl</title><meta name="title" content="Tutorial · LsqFit.jl"/><meta property="og:title" content="Tutorial · LsqFit.jl"/><meta property="twitter:title" content="Tutorial · LsqFit.jl"/><meta name="description" content="Documentation for LsqFit.jl."/><meta property="og:description" content="Documentation for LsqFit.jl."/><meta property="twitter:description" content="Documentation for LsqFit.jl."/><meta property="og:url" content="https://julianlsolvers.github.io/LsqFit.jl/stable/tutorial/"/><meta property="twitter:url" content="https://julianlsolvers.github.io/LsqFit.jl/stable/tutorial/"/><link rel="canonical" href="https://julianlsolvers.github.io/LsqFit.jl/stable/tutorial/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">LsqFit.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li class="is-active"><a class="tocitem" href>Tutorial</a><ul class="internal"><li><a class="tocitem" href="#Introduction-to-Nonlinear-Regression"><span>Introduction to Nonlinear Regression</span></a></li><li><a class="tocitem" href="#Jacobian-Calculation"><span>Jacobian Calculation</span></a></li><li><a class="tocitem" href="#Linear-Approximation"><span>Linear Approximation</span></a></li><li><a class="tocitem" href="#Goodness-of-Fit"><span>Goodness of Fit</span></a></li><li><a class="tocitem" href="#Weighted-Least-Squares"><span>Weighted Least Squares</span></a></li><li><a class="tocitem" href="#General-Least-Squares"><span>General Least Squares</span></a></li><li><a class="tocitem" href="#Estimate-the-Optimal-Weight"><span>Estimate the Optimal Weight</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../api/">API References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Tutorial</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tutorial</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaNLSolvers/LsqFit.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaNLSolvers/LsqFit.jl/blob/master/docs/src/tutorial.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Tutorial"><a class="docs-heading-anchor" href="#Tutorial">Tutorial</a><a id="Tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#Tutorial" title="Permalink"></a></h1><h2 id="Introduction-to-Nonlinear-Regression"><a class="docs-heading-anchor" href="#Introduction-to-Nonlinear-Regression">Introduction to Nonlinear Regression</a><a id="Introduction-to-Nonlinear-Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction-to-Nonlinear-Regression" title="Permalink"></a></h2><p>Assume that, for the <span>$i$</span>th observation, the relationship between independent variable <span>$\mathbf{x_i}=\begin{bmatrix} x_{1i},\, x_{2i},\, \ldots\, x_{pi}\ \end{bmatrix}&#39;$</span> and dependent variable <span>$Y_i$</span> follows:</p><p class="math-container">\[Y_i = m(\mathbf{x_i}, \boldsymbol{\gamma}) + \epsilon_i\]</p><p>where <span>$m$</span> is a non-linear model function depends on the independent variable <span>$\mathbf{x_i}$</span> and the parameter vector <span>$\boldsymbol{\gamma}$</span>. In order to find the parameter <span>$\boldsymbol{\gamma}$</span> that &quot;best&quot; fit our data, we choose the parameter <span>${\boldsymbol{\gamma}}$</span> which minimizes the sum of squared residuals from our data, i.e. solves the problem:</p><p class="math-container">\[\underset{\boldsymbol{\gamma}}{\mathrm{min}} \quad s(\boldsymbol{\gamma})= \sum_{i=1}^{n} [m(\mathbf{x_i}, \boldsymbol{\gamma}) - y_i]^2\]</p><p>Given that the function <span>$m$</span> is non-linear, there&#39;s no analytical solution for the best <span>$\boldsymbol{\gamma}$</span>. We have to use computational tools, which is <code>LsqFit.jl</code> in this tutorial, to find the least squares solution.</p><p>One example of non-linear model is the exponential model, which takes a one-element predictor variable <span>$t$</span>. The model function is:</p><p class="math-container">\[m(t, \boldsymbol{\gamma}) = \gamma_1 \exp(\gamma_2 t)\]</p><p>and the model becomes:</p><p class="math-container">\[Y_i = \gamma_1 \exp(\gamma_2 t_i) + \epsilon_i\]</p><p>To fit data using <code>LsqFit.jl</code>, pass the defined model function (<code>m</code>), data (<code>tdata</code> and <code>ydata</code>) and the initial parameter value (<code>p0</code>) to <code>curve_fit()</code>. For now, <code>LsqFit.jl</code> only supports the Levenberg Marquardt algorithm.</p><pre><code class="language-julia hljs">julia&gt; # t: array of independent variables
julia&gt; # p: array of model parameters
julia&gt; m(t, p) = p[1] * exp.(p[2] * t)
julia&gt; p0 = [0.5, 0.5]
julia&gt; fit = curve_fit(m, tdata, ydata, p0)</code></pre><p>It will return a composite type <code>LsqFitResult</code>, with some interesting values:</p><p>*	<code>dof(fit)</code>: degrees of freedom *	<code>coef(fit)</code>: best fit parameters *	<code>fit.resid</code>: vector of residuals *	<code>fit.jacobian</code>: estimated Jacobian at the solution</p><h2 id="Jacobian-Calculation"><a class="docs-heading-anchor" href="#Jacobian-Calculation">Jacobian Calculation</a><a id="Jacobian-Calculation-1"></a><a class="docs-heading-anchor-permalink" href="#Jacobian-Calculation" title="Permalink"></a></h2><p>The Jacobian <span>$J_f(\mathbf{x})$</span> of a vector function <span>$f(\mathbf{x}): \mathbb{R}_m \to \mathbb{R}_n$</span> is deﬁned as the matrix with elements:</p><p class="math-container">\[[J_f(\mathbf{x})]_{ij} = \frac{\partial f_i(\mathbf{x})}{\partial x_j}\]</p><p>The matrix is therefore:</p><p class="math-container">\[J_f(\mathbf{x}) = \begin{bmatrix}
                \frac{\partial f_1}{\partial x_1}&amp;\frac{\partial f_1}{\partial x_2}&amp;\dots&amp;\frac{\partial f_1}{\partial x_m}\\
                \frac{\partial f_2}{\partial x_1}&amp;\frac{\partial f_2}{\partial x_2}&amp;\dots&amp;\frac{\partial f_2}{\partial x_m}\\
                \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
                \frac{\partial f_n}{\partial x_1}&amp;\frac{\partial f_n}{\partial x_2}&amp;\dots&amp;\frac{\partial f_n}{\partial x_m}\\
                \end{bmatrix}\]</p><p>The Jacobian of the exponential model function with respect to <span>$\boldsymbol{\gamma}$</span> is:</p><p class="math-container">\[J_m(t, \boldsymbol{\gamma}) = \begin{bmatrix}
            \frac{\partial m}{\partial \gamma_1} &amp;
            \frac{\partial m}{\partial \gamma_2} \\
            \end{bmatrix}
          = \begin{bmatrix}
            \exp(\gamma_2 t) &amp;
            t \gamma_1 \exp(\gamma_2 t) \\
            \end{bmatrix}\]</p><p>By default, the finite differences is used (see <a href="https://github.com/JuliaNLSolvers/NLSolversBase.jl">NLSolversBase.jl</a> for more information), is used to approximate the Jacobian for the data fitting algorithm and covariance computation. Alternatively, a function which calculates the Jacobian can be supplied to <code>curve_fit()</code> for faster and/or more accurate results.</p><pre><code class="language-Julia hljs">function j_m(t,p)
    J = Array{Float64}(length(t),length(p))
    J[:,1] = exp.(p[2] .* t)       #df/dp[1]
    J[:,2] = t .* p[1] .* J[:,1]   #df/dp[2]
    J
end

fit = curve_fit(m, j_m, tdata, ydata, p0)</code></pre><h2 id="Linear-Approximation"><a class="docs-heading-anchor" href="#Linear-Approximation">Linear Approximation</a><a id="Linear-Approximation-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-Approximation" title="Permalink"></a></h2><p>The non-linear function <span>$m$</span> can be approximated as a linear function by Talor expansion:</p><p class="math-container">\[m(\mathbf{x_i}, \boldsymbol{\gamma}+\boldsymbol{h}) \approx m(\mathbf{x_i}, \boldsymbol{\gamma}) +  \nabla m(\mathbf{x_i}, \boldsymbol{\gamma})&#39;\boldsymbol{h}\]</p><p>where <span>$\boldsymbol{\gamma}$</span> is a fixed vector, <span>$\boldsymbol{h}$</span> is a very small-valued vector and <span>$\nabla m(\mathbf{x_i}, \boldsymbol{\gamma})$</span> is the gradient at <span>$\mathbf{x_i}$</span>.</p><p>Consider the residual vector functon <span>$r({\boldsymbol{\gamma}})=\begin{bmatrix}                           r_1({\boldsymbol{\gamma}}) \\
                          r_2({\boldsymbol{\gamma}}) \\
                          \vdots\\
                          r_n({\boldsymbol{\gamma}})                           \end{bmatrix}$</span> with entries:</p><p class="math-container">\[r_i({\boldsymbol{\gamma}}) = m(\mathbf{x_i}, {\boldsymbol{\gamma}}) - Y_i\]</p><p>Each entry&#39;s linear approximation can hence be written as:</p><p class="math-container">\[\begin{align}
r_i({\boldsymbol{\gamma}}+\boldsymbol{h}) &amp;= m(\mathbf{x_i}, \boldsymbol{\gamma}+\boldsymbol{h}) - Y_i\\
&amp;\approx m(\mathbf{x_i}, \boldsymbol{\gamma}) + \nabla m(\mathbf{x_i}, \boldsymbol{\gamma})&#39;h - Y_i\\
&amp;= r_i({\boldsymbol{\gamma}}) + \nabla m(\mathbf{x_i}, \boldsymbol{\gamma})&#39;h
\end{align}\]</p><p>Since the <span>$i$</span>th row of <span>$J(\boldsymbol{\gamma})$</span> equals the transpose of the gradient of <span>$m(\mathbf{x_i}, \boldsymbol{\gamma})$</span>, the vector function <span>$r({\boldsymbol{\gamma}}+\boldsymbol{h})$</span> can be approximated as:</p><p class="math-container">\[r({\boldsymbol{\gamma}}+\boldsymbol{h}) \approx r({\boldsymbol{\gamma}}) + J(\boldsymbol{\gamma})h\]</p><p>which is a linear function on <span>$\boldsymbol{h}$</span> since <span>${\boldsymbol{\gamma}}$</span> is a fixed vector.</p><h2 id="Goodness-of-Fit"><a class="docs-heading-anchor" href="#Goodness-of-Fit">Goodness of Fit</a><a id="Goodness-of-Fit-1"></a><a class="docs-heading-anchor-permalink" href="#Goodness-of-Fit" title="Permalink"></a></h2><p>The linear approximation of the non-linear least squares problem leads to the approximation of the covariance matrix of each parameter, from which we can perform regression analysis.</p><p>Consider a least squares solution <span>$\boldsymbol{\gamma}^*$</span>, which is a local minimizer of the non-linear problem:</p><p class="math-container">\[\boldsymbol{\gamma}^* = \underset{\boldsymbol{\gamma}}{\mathrm{arg\,min}} \ \sum_{i=1}^{n} [m(\mathbf{x_i}, \boldsymbol{\gamma}) - y_i]^2\]</p><p>Set <span>$\boldsymbol{\gamma}^*$</span> as the fixed point in linear approximation, <span>$r({\boldsymbol{\gamma^*}}) = r$</span> and <span>$J(\boldsymbol{\gamma^*}) = J$</span>. A parameter vector near <span>$\boldsymbol{\gamma}^*$</span> can be expressed as <span>$\boldsymbol{\gamma}=\boldsymbol{\gamma^*} + h$</span>. The local approximation for the least squares problem is:</p><p class="math-container">\[\underset{\boldsymbol{\gamma}}{\mathrm{min}} \quad s(\boldsymbol{\gamma})=s(\boldsymbol{\gamma}^*+\boldsymbol{h}) \approx [Jh + r]&#39;[Jh + r]\]</p><p>which is essentially the linear least squares problem:</p><p class="math-container">\[\underset{\boldsymbol{\beta}}{\mathrm{min}} \quad [X\beta-Y]&#39;[X\beta-Y]\]</p><p>where <span>$X=J$</span>, <span>$\beta=\boldsymbol{h}$</span> and <span>$Y=-r({\boldsymbol{\gamma}})$</span>. Solve the equation where the partial derivatives equal to <span>$0$</span>, the analytical solution is:</p><p class="math-container">\[\hat{\boldsymbol{h}}=\hat{\boldsymbol{\gamma}}-\boldsymbol{\gamma}^*\approx-[J&#39;J]^{-1}J&#39;r\]</p><p>The covariance matrix for the analytical solution is:</p><p class="math-container">\[\mathbf{Cov}(\hat{\boldsymbol{\gamma}}) = \mathbf{Cov}(\boldsymbol{h}) = [J&#39;J]^{-1}J&#39;\mathbf{E}(rr&#39;)J[J&#39;J]^{-1}\]</p><p>Note that <span>$r$</span> is the residual vector at the best fit point <span>$\boldsymbol{\gamma^*}$</span>, with entries <span>$r_i = Y_i - m(\mathbf{x_i}, \boldsymbol{\gamma^*})=\epsilon_i$</span>. <span>$\hat{\boldsymbol{\gamma}}$</span> is very close to <span>$\boldsymbol{\gamma^*}$</span> and therefore can be replaced by <span>$\boldsymbol{\gamma^*}$</span>.</p><p class="math-container">\[\mathbf{Cov}(\boldsymbol{\gamma}^*) \approx \mathbf{Cov}(\hat{\boldsymbol{\gamma}})\]</p><p>Assume the errors in each sample are independent, normal distributed with zero mean and same variance, i.e. <span>$\epsilon \sim N(0, \sigma^2I)$</span>, the covariance matrix from the linear approximation is therefore:</p><p class="math-container">\[\mathbf{Cov}(\boldsymbol{\gamma}^*) = [J&#39;J]^{-1}J&#39;\mathbf{Cov}(\epsilon)J[J&#39;J]^{-1} = \sigma^2[J&#39;J]^{-1}\]</p><p>where <span>$\sigma^2$</span> could be estimated as residual sum of squares devided by degrees of freedom:</p><p class="math-container">\[\hat{\sigma}^2=\frac{s(\boldsymbol{\gamma}^*)}{n-p}\]</p><p>In <code>LsqFit.jl</code>, the covariance matrix calculation uses QR decomposition to <a href="http://www.seas.ucla.edu/~vandenbe/133A/lectures/ls.pdf">be more computationally stable</a>, which has the form:</p><p class="math-container">\[\mathbf{Cov}(\boldsymbol{\gamma}^*) = \hat{\sigma}^2 \mathrm{R}^{-1}(\mathrm{R}^{-1})&#39;\]</p><p><code>vcov()</code> computes the covariance matrix of fit:</p><pre><code class="language-Julia hljs">julia&gt; cov = vcov(fit)
2×2 Array{Float64,2}:
 0.000116545  0.000174633
 0.000174633  0.00258261</code></pre><p>The standard error is then the square root of each diagonal elements of the covariance matrix. <code>stderror()</code> returns the standard error of each parameter:</p><pre><code class="language-Julia hljs">julia&gt; se = stderror(fit)
2-element Array{Float64,1}:
 0.0114802
 0.0520416</code></pre><p><code>margin_error()</code> computes the product of standard error and the critical value of each parameter at a certain significance level (default is 5%) from t-distribution. The margin of error at 10% significance level can be computed by:</p><pre><code class="language-Julia hljs">julia&gt; margin_of_error = margin_error(fit, 0.1)
2-element Array{Float64,1}:
 0.0199073
 0.0902435</code></pre><p><code>confint()</code> returns the confidence interval of each parameter at certain significance level, which is essentially the estimate value ± margin of error. To get the confidence interval at 10% significance level, run:</p><pre><code class="language-Julia hljs">julia&gt; confidence_intervals = confint(fit; level=0.9)
2-element Array{Tuple{Float64,Float64},1}:
 (0.976316, 1.01613)
 (1.91047, 2.09096)</code></pre><h2 id="Weighted-Least-Squares"><a class="docs-heading-anchor" href="#Weighted-Least-Squares">Weighted Least Squares</a><a id="Weighted-Least-Squares-1"></a><a class="docs-heading-anchor-permalink" href="#Weighted-Least-Squares" title="Permalink"></a></h2><p><code>curve_fit()</code> also accepts weight parameter (<code>wt</code>) to perform Weighted Least Squares and General Least Squares, where the parameter <span>$\boldsymbol{\gamma}^*$</span> minimizes the weighted residual sum of squares.</p><p>Weight parameter (<code>wt</code>) is an array or a matrix of weights for each sample. To perform Weighted Least Squares, pass the weight array <code>[w_1, w_2, ..., w_n]</code> or the weight matrix <code>W</code>:</p><p class="math-container">\[\mathbf{W} = \begin{bmatrix}
    w_1    &amp; 0      &amp; \cdots &amp; 0\\
    0      &amp; w_2    &amp; \cdots &amp; 0\\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    0      &amp; 0    &amp; \cdots &amp; w_n\\
    \end{bmatrix}\]</p><p>The weighted least squares problem becomes:</p><p class="math-container">\[\underset{\boldsymbol{\gamma}}{\mathrm{min}} \quad s(\boldsymbol{\gamma})= \sum_{i=1}^{n} w_i[m(\mathbf{x_i}, \boldsymbol{\gamma}) - Y_i]^2\]</p><p>in matrix form:</p><p class="math-container">\[\underset{\boldsymbol{\gamma}}{\mathrm{min}} \quad s(\boldsymbol{\gamma})= r(\boldsymbol{\gamma})&#39;Wr(\boldsymbol{\gamma})\]</p><p>where <span>$r({\boldsymbol{\gamma}})=\begin{bmatrix}                           r_1({\boldsymbol{\gamma}}) \\
                          r_2({\boldsymbol{\gamma}}) \\
                          \vdots\\
                          r_n({\boldsymbol{\gamma}})                           \end{bmatrix}$</span> is a residual vector function with entries:</p><p class="math-container">\[r_i({\boldsymbol{\gamma}}) = m(\mathbf{x_i}, {\boldsymbol{\gamma}}) - Y_i\]</p><p>The algorithm in <code>LsqFit.jl</code> will then provide a least squares solution <span>$\boldsymbol{\gamma}^*$</span>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>In <code>LsqFit.jl</code>, the residual function passed to <code>levenberg_marquardt()</code> is in different format, if the weight is a vector:</p><pre><code class="language-julia hljs">r(p) = sqrt.(wt) .* ( model(xpts, p) - ydata )
lmfit(r, g, p0, wt; kwargs...)</code></pre><p class="math-container">\[r_i({\boldsymbol{\gamma}}) = \sqrt{w_i} \cdot [m(\mathbf{x_i}, {\boldsymbol{\gamma}}) - Y_i]\]</p><p>Cholesky decomposition, which is effectively a sqrt of a matrix, will be performed if the weight is a matrix:</p><pre><code class="language-julia hljs">u = chol(wt)
r(p) = u * ( model(xpts, p) - ydata )
lmfit(r, p0, wt; kwargs...)</code></pre><p class="math-container">\[r_i({\boldsymbol{\gamma}}) = \sqrt{w_i} \cdot [m(\mathbf{x_i}, {\boldsymbol{\gamma}}) - Y_i]\]</p><p>The solution will be the same as the least squares problem mentioned in the tutorial.</p></div></div><p>Set <span>$r({\boldsymbol{\gamma^*}}) = r$</span> and <span>$J(\boldsymbol{\gamma^*}) = J$</span>, the linear approximation of the weighted least squares problem is then:</p><p class="math-container">\[\underset{\boldsymbol{\gamma}}{\mathrm{min}} \quad s(\boldsymbol{\gamma}) = s(\boldsymbol{\gamma}^* + \boldsymbol{h}) \approx [J\boldsymbol{h}+r]&#39;W[J\boldsymbol{h}+r]\]</p><p>The analytical solution to the linear approximation is:</p><p class="math-container">\[\hat{\boldsymbol{h}}=\hat{\boldsymbol{\gamma}}-\boldsymbol{\gamma}^*\approx-[J&#39;WJ]^{-1}J&#39;Wr\]</p><p>Assume the errors in each sample are independent, normal distributed with zero mean and <strong>different</strong> variances (heteroskedastic error), i.e. <span>$\epsilon \sim N(0, \Sigma)$</span>, where:</p><p class="math-container">\[\Sigma = \begin{bmatrix}
         \sigma_1^2    &amp; 0      &amp; \cdots &amp; 0\\
         0      &amp; \sigma_2^2    &amp; \cdots &amp; 0\\
         \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
         0      &amp; 0    &amp; \cdots &amp; \sigma_n^2\\
         \end{bmatrix}\]</p><p>We know the error variance and we set the weight as the inverse of the variance (the optimal weight), i.e. <span>$W = \Sigma^{-1}$</span>:</p><p class="math-container">\[\mathbf{W} =  \begin{bmatrix}
              w_1    &amp; 0      &amp; \cdots &amp; 0\\
              0      &amp; w_2    &amp; \cdots &amp; 0\\
              \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
              0      &amp; 0    &amp; \cdots &amp; w_n\\
              \end{bmatrix}
           =  \begin{bmatrix}
               \frac{1}{\sigma_1^2}    &amp; 0      &amp; \cdots &amp; 0\\
               0      &amp; \frac{1}{\sigma_2^2}    &amp; \cdots &amp; 0\\
               \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
               0      &amp; 0    &amp; \cdots &amp; \frac{1}{\sigma_n^2}\\
               \end{bmatrix}\]</p><p>The covariance matrix is now:</p><p class="math-container">\[{Cov}(\boldsymbol{\gamma}^*) \approx  [J&#39;WJ]^{-1}J&#39;W \Sigma W&#39;J[J&#39;W&#39;J]^{-1} = [J&#39;WJ]^{-1}\]</p><p>If we only know <strong>the relative ratio of different variances</strong>, i.e. <span>$\epsilon \sim N(0, \sigma^2W^{-1})$</span>, the covariance matrix will be:</p><p class="math-container">\[\mathbf{Cov}(\boldsymbol{\gamma}^*) = \sigma^2[J&#39;WJ]^{-1}\]</p><p>where <span>$\sigma^2$</span> is estimated. In this case, if we set <span>$W = I$</span>, the result will be the same as the unweighted version. However, <code>curve_fit()</code> currently <strong>does not support</strong> this implementation. <code>curve_fit()</code> assumes the weight as the inverse of <strong>the error covariance matrix</strong> rather than <strong>the ratio of error covariance matrix</strong>, i.e. the covariance of the estimated parameter is calculated as <code>covar = inv(J&#39;*fit.wt*J)</code>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Passing vector of ones as the weight vector will cause mistakes in covariance estimation.</p></div></div><p>Pass the vector of <code>1 ./ var(ε)</code> or the matrix <code>inv(covar(ε))</code> as the weight parameter (<code>wt</code>) to the function <code>curve_fit()</code>:</p><pre><code class="language-Julia hljs">julia&gt; wt = inv(cov_ε)
julia&gt; fit = curve_fit(m, tdata, ydata, wt, p0)
julia&gt; cov = vcov(fit)</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If the weight matrix is not a diagonal matrix, General Least Squares will be performed.</p></div></div><h2 id="General-Least-Squares"><a class="docs-heading-anchor" href="#General-Least-Squares">General Least Squares</a><a id="General-Least-Squares-1"></a><a class="docs-heading-anchor-permalink" href="#General-Least-Squares" title="Permalink"></a></h2><p>Assume the errors in each sample are <strong>correlated</strong>, normal distributed with zero mean and <strong>different</strong> variances (heteroskedastic and autocorrelated error), i.e. <span>$\epsilon \sim N(0, \Sigma)$</span>.</p><p>Set the weight matrix as the inverse of the error covariance matrix (the optimal weight), i.e. <span>$W = \Sigma^{-1}$</span>, we will get the parameter covariance matrix:</p><p class="math-container">\[\mathbf{Cov}(\boldsymbol{\gamma}^*) \approx  [J&#39;WJ]^{-1}J&#39;W \Sigma W&#39;J[J&#39;W&#39;J]^{-1} = [J&#39;WJ]^{-1}\]</p><p>Pass the matrix <code>inv(covar(ε))</code> as the weight parameter (<code>wt</code>) to the function <code>curve_fit()</code>:</p><pre><code class="language-Julia hljs">julia&gt; wt = 1 ./ yvar
julia&gt; fit = curve_fit(m, tdata, ydata, wt, p0)
julia&gt; cov = vcov(fit)</code></pre><h2 id="Estimate-the-Optimal-Weight"><a class="docs-heading-anchor" href="#Estimate-the-Optimal-Weight">Estimate the Optimal Weight</a><a id="Estimate-the-Optimal-Weight-1"></a><a class="docs-heading-anchor-permalink" href="#Estimate-the-Optimal-Weight" title="Permalink"></a></h2><p>In most cases, the variances of errors are unknown. To perform Weighted Least Square, we need estimate the variances of errors first, which is the squared residual of <span>$i$</span>th sample:</p><p class="math-container">\[\widehat{\mathbf{Var}(\epsilon_i)} = \widehat{\mathbf{E}(\epsilon_i \epsilon_i)} = r_i(\boldsymbol{\gamma}^*)\]</p><p>Unweighted fitting (OLS) will return the residuals we need, since the estimator of OLS is unbiased. Then pass the reciprocal of the residuals as the estimated optimal weight to perform Weighted Least Squares:</p><pre><code class="language-Julia hljs">julia&gt; fit_OLS = curve_fit(m, tdata, ydata, p0)
julia&gt; wt = 1 ./ fit_OLS.resid
julia&gt; fit_WLS = curve_fit(m, tdata, ydata, wt, p0)
julia&gt; cov = vcov(fit_WLS)</code></pre><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><p>Hansen, P. C., Pereyra, V. and Scherer, G. (2013) Least squares data fitting with applications. Baltimore, Md: Johns Hopkins University Press, p. 147-155.</p><p>Kutner, M. H. et al. (2005) Applied Linear statistical models.</p><p>Weisberg, S. (2014) Applied linear regression. Fourth edition. Hoboken, NJ: Wiley (Wiley series in probability and statistics).</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../getting_started/">« Getting Started</a><a class="docs-footer-nextpage" href="../api/">API References »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.1.0 on <span class="colophon-date" title="Sunday 8 October 2023 18:15">Sunday 8 October 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
